{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "url feature extraction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhX5CsaRiL7C"
      },
      "outputs": [],
      "source": [
        "import ipaddress\n",
        "import re\n",
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "import socket\n",
        "import requests\n",
        "from googlesearch import search\n",
        "import whois\n",
        "from datetime import datetime\n",
        "import time\n",
        "from dateutil.parser import parse as date_parse\n",
        "\n",
        "# Calculates number of months\n",
        "def diff_month(d1, d2):\n",
        "    return (d1.year - d2.year) * 12 + d1.month - d2.month\n",
        "\n",
        "# Generate data set by extracting the features from the URL\n",
        "def generate_data_set(url):\n",
        "\n",
        "    data_set = []\n",
        "\n",
        "    # Converts the given URL into standard format\n",
        "    if not re.match(r\"^https?\", url):\n",
        "        url = \"http://\" + url\n",
        "\n",
        "\n",
        "    # Stores the response of the given URL\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    except:\n",
        "        response = \"\"\n",
        "        soup = -999\n",
        "\n",
        "\n",
        "    # Extracts domain from the given URL\n",
        "    domain = re.findall(r\"://([^/]+)/?\", url)[0]\n",
        "    if re.match(r\"^www.\",domain):\n",
        "\t       domain = domain.replace(\"www.\",\"\")\n",
        "\n",
        "    # Requests all the information about the domain\n",
        "    whois_response = whois.whois(domain)\n",
        "\n",
        "    rank_checker_response = requests.post(\"https://www.checkpagerank.net/index.php\", {\n",
        "        \"name\": domain\n",
        "    })\n",
        "\n",
        "    # Extracts global rank of the website\n",
        "    try:\n",
        "        global_rank = int(re.findall(r\"Global Rank: ([0-9]+)\", rank_checker_response.text)[0])\n",
        "    except:\n",
        "        global_rank = -1\n",
        "\n",
        "    # 1.having_IP_Address\n",
        "    try:\n",
        "        ipaddress.ip_address(url)\n",
        "        data_set.append(-1)\n",
        "    except:\n",
        "        data_set.append(1)\n",
        "\n",
        "    # 2.URL_Length\n",
        "    if len(url) < 54:\n",
        "        data_set.append(1)\n",
        "    elif len(url) >= 54 and len(url) <= 75:\n",
        "        data_set.append(0)\n",
        "    else:\n",
        "        data_set.append(-1)\n",
        "\n",
        "    # 3.Shortining_Service\n",
        "    match=re.search('bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|'\n",
        "                    'yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|'\n",
        "                    'short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|'\n",
        "                    'doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|'\n",
        "                    'db\\.tt|qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|'\n",
        "                    'q\\.gs|is\\.gd|po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|'\n",
        "                    'x\\.co|prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|tr\\.im|link\\.zip\\.net',url)\n",
        "    if match:\n",
        "        data_set.append(match)\n",
        "    else:\n",
        "        data_set.append(1)\n",
        "\n",
        "    # 4.having_At_Symbol\n",
        "    if re.findall(\"@\", url):\n",
        "        data_set.append(url)\n",
        "    else:\n",
        "        data_set.append(1)\n",
        "\n",
        "    # 5.double_slash_redirecting\n",
        "    list=[x.start(0) for x in re.finditer('//', url)]\n",
        "    if list[len(list)-1]>6:\n",
        "        data_set.append(list)\n",
        "    else:\n",
        "        data_set.append(1)\n",
        "\n",
        "    # 6.Prefix_Suffix\n",
        "    if re.findall(r\"https?://[^\\-]+-[^\\-]+/\", url):\n",
        "        data_set.append(url)\n",
        "    else:\n",
        "        data_set.append(1)\n",
        "\n",
        "    # 7.having_Sub_Domain\n",
        "    if len(re.findall(\"\\.\", url)) == 1:\n",
        "        data_set.append(1)\n",
        "    elif len(re.findall(\"\\.\", url)) == 2:\n",
        "        data_set.append(0)\n",
        "    else:\n",
        "        data_set.append(-1)\n",
        "\n",
        "    # 8.SSLfinal_State\n",
        "    try:\n",
        "        if response.text:\n",
        "            data_set.append(1)\n",
        "    except:\n",
        "        data_set.append(-1)\n",
        "\n",
        "    # 9.Domain_registeration_length\n",
        "    expiration_date = whois_response.expiration_date\n",
        "    registration_length = 0\n",
        "    try:\n",
        "        expiration_date = min(expiration_date)\n",
        "        today = time.strftime('%Y-%m-%d')\n",
        "        today = datetime.strptime(today, '%Y-%m-%d')\n",
        "        registration_length = abs((expiration_date - today).days)\n",
        "\n",
        "        if registration_length / 365 <= 1:\n",
        "            data_set.append(-1)\n",
        "        else:\n",
        "            data_set.append(1)\n",
        "    except:\n",
        "        data_set.append(-1)\n",
        "\n",
        "    # 10.Favicon\n",
        "    if soup == -999:\n",
        "        data_set.append(-1)\n",
        "    else:\n",
        "        try:\n",
        "            for head in soup.find_all('head'):\n",
        "                for head.link in soup.find_all('link', href=True):\n",
        "                    dots = [x.start(0) for x in re.finditer('\\.', head.link['href'])]\n",
        "                    if url in head.link['href'] or len(dots) == 1 or domain in head.link['href']:\n",
        "                        data_set.append(1)\n",
        "                        raise StopIteration\n",
        "                    else:\n",
        "                        data_set.append(-1)\n",
        "                        raise StopIteration\n",
        "        except StopIteration:\n",
        "            pass\n",
        "\n",
        "    #11. port\n",
        "    try:\n",
        "        port = domain.split(\":\")[1]\n",
        "        if port:\n",
        "            data_set.append(-1)\n",
        "        else:\n",
        "            data_set.append(1)\n",
        "    except:\n",
        "        data_set.append(1)\n",
        "\n",
        "    #12. HTTPS_token\n",
        "    if re.findall(r\"^https://\", url):\n",
        "        data_set.append(1)\n",
        "    else:\n",
        "        data_set.append(-1)\n",
        "\n",
        "    #13. Request_URL\n",
        "    i = 0\n",
        "    success = 0\n",
        "    if soup == -999:\n",
        "        data_set.append(-1)\n",
        "    else:\n",
        "        for img in soup.find_all('img', src= True):\n",
        "           dots= [x.start(0) for x in re.finditer('\\.', img['src'])]\n",
        "           if url in img['src'] or domain in img['src'] or len(dots)==1:\n",
        "              success = success + 1\n",
        "           i=i+1\n",
        "\n",
        "        for audio in soup.find_all('audio', src= True):\n",
        "           dots = [x.start(0) for x in re.finditer('\\.', audio['src'])]\n",
        "           if url in audio['src'] or domain in audio['src'] or len(dots)==1:\n",
        "              success = success + 1\n",
        "           i=i+1\n",
        "\n",
        "        for embed in soup.find_all('embed', src= True):\n",
        "           dots=[x.start(0) for x in re.finditer('\\.',embed['src'])]\n",
        "           if url in embed['src'] or domain in embed['src'] or len(dots)==1:\n",
        "              success = success + 1\n",
        "           i=i+1\n",
        "\n",
        "        for iframe in soup.find_all('iframe', src= True):\n",
        "           dots=[x.start(0) for x in re.finditer('\\.',iframe['src'])]\n",
        "           if url in iframe['src'] or domain in iframe['src'] or len(dots)==1:\n",
        "              success = success + 1\n",
        "           i=i+1\n",
        "\n",
        "        try:\n",
        "           percentage = success/float(i) * 100\n",
        "           if percentage < 22.0 :\n",
        "              dataset.append(1)\n",
        "           elif((percentage >= 22.0) and (percentage < 61.0)) :\n",
        "              data_set.append(0)\n",
        "           else :\n",
        "              data_set.append(-1)\n",
        "        except:\n",
        "            data_set.append(1)\n",
        "\n",
        "\n",
        "\n",
        "    #14. URL_of_Anchor\n",
        "    percentage = 0\n",
        "    i = 0\n",
        "    unsafe=0\n",
        "    if soup == -999:\n",
        "        data_set.append(-1)\n",
        "    else:\n",
        "        for a in soup.find_all('a', href=True):\n",
        "        # 2nd condition was 'JavaScript ::void(0)' but we put JavaScript because the space between javascript and :: might not be\n",
        "                # there in the actual a['href']\n",
        "            if \"#\" in a['href'] or \"javascript\" in a['href'].lower() or \"mailto\" in a['href'].lower() or not (url in a['href'] or domain in a['href']):\n",
        "                unsafe = unsafe + 1\n",
        "            i = i + 1\n",
        "\n",
        "\n",
        "        try:\n",
        "            percentage = unsafe / float(i) * 100\n",
        "        except:\n",
        "            data_set.append(1)\n",
        "\n",
        "        if percentage < 31.0:\n",
        "            data_set.append(1)\n",
        "        elif ((percentage >= 31.0) and (percentage < 67.0)):\n",
        "            data_set.append(0)\n",
        "        else:\n",
        "            data_set.append(-1)\n",
        "\n",
        "    #15. Links_in_tags\n",
        "    i=0\n",
        "    success =0\n",
        "    if soup == -999:\n",
        "        data_set.append(-1)\n",
        "    else:\n",
        "        for link in soup.find_all('link', href= True):\n",
        "           dots=[x.start(0) for x in re.finditer('\\.',link['href'])]\n",
        "           if url in link['href'] or domain in link['href'] or len(dots)==1:\n",
        "              success = success + 1\n",
        "           i=i+1\n",
        "\n",
        "        for script in soup.find_all('script', src= True):\n",
        "           dots=[x.start(0) for x in re.finditer('\\.',script['src'])]\n",
        "           if url in script['src'] or domain in script['src'] or len(dots)==1 :\n",
        "              success = success + 1\n",
        "           i=i+1\n",
        "        try:\n",
        "            percentage = success / float(i) * 100\n",
        "        except:\n",
        "            data_set.append(1)\n",
        "\n",
        "        if percentage < 17.0 :\n",
        "           data_set.append(1)\n",
        "        elif((percentage >= 17.0) and (percentage < 81.0)) :\n",
        "           data_set.append(0)\n",
        "        else :\n",
        "           data_set.append(-1)\n",
        "\n",
        "        #16. SFH\n",
        "        for form in soup.find_all('form', action= True):\n",
        "           if form['action'] ==\"\" or form['action'] == \"about:blank\" :\n",
        "              data_set.append(-1)\n",
        "              break\n",
        "           elif url not in form['action'] and domain not in form['action']:\n",
        "               data_set.append(0)\n",
        "               break\n",
        "           else:\n",
        "                 data_set.append(1)\n",
        "                 break\n",
        "\n",
        "    #17. Submitting_to_email\n",
        "    if response == \"\":\n",
        "        data_set.append(-1)\n",
        "    else:\n",
        "        if re.findall(r\"[mail\\(\\)|mailto:?]\", response.text):\n",
        "            data_set.append(1)\n",
        "        else:\n",
        "            data_set.append(-1)\n",
        "\n",
        "    #18. Abnormal_URL\n",
        "    if response == \"\":\n",
        "        data_set.append(response)\n",
        "    else:\n",
        "        if response.text == \"\":\n",
        "            data_set.append(1)\n",
        "        else:\n",
        "            data_set.append(response.text)\n",
        "\n",
        "    #19. Redirect\n",
        "    if response == \"\":\n",
        "        data_set.append(response)\n",
        "    else:\n",
        "        if len(response.history) <= 1:\n",
        "            data_set.append(-1)\n",
        "        elif len(response.history) <= 4:\n",
        "            data_set.append(0)\n",
        "        else:\n",
        "            data_set.append(1)\n",
        "\n",
        "    #20. on_mouseover\n",
        "    if response == \"\" :\n",
        "        data_set.append(-1)\n",
        "    else:\n",
        "        if re.findall(\"<script>.+onmouseover.+</script>\", response.text):\n",
        "            data_set.append(1)\n",
        "        else:\n",
        "            data_set.append(-1)\n",
        "\n",
        "    #21. RightClick\n",
        "    if response == \"\":\n",
        "        data_set.append(-1)\n",
        "    else:\n",
        "        if re.findall(r\"event.button ?== ?2\", response.text):\n",
        "            data_set.append(1)\n",
        "        else:\n",
        "            data_set.append(-1)\n",
        "\n",
        "    #22. popUpWidnow\n",
        "    if response == \"\":\n",
        "        data_set.append(-1)\n",
        "    else:\n",
        "        if re.findall(r\"alert\\(\", response.text):\n",
        "            data_set.append(1)\n",
        "        else:\n",
        "            data_set.append(-1)\n",
        "\n",
        "    #23. Iframe\n",
        "    if response == \"\":\n",
        "        data_set.append(-1)\n",
        "    else:\n",
        "        if re.findall(r\"[<iframe>|<frameBorder>]\", response.text):\n",
        "            data_set.append(1)\n",
        "        else:\n",
        "            data_set.append(-1)\n",
        "\n",
        "    #24. age_of_domain\n",
        "    if response == \"\":\n",
        "        data_set.append(-1)\n",
        "    else:\n",
        "        try:\n",
        "            registration_date = re.findall(r'Registration Date:</div><div class=\"df-value\">([^<]+)</div>', whois_response.text)[0]\n",
        "            if diff_month(date.today(), date_parse(registration_date)) >= 6:\n",
        "                data_set.append(-1)\n",
        "            else:\n",
        "                data_set.append(1)\n",
        "        except:\n",
        "            data_set.append(1)\n",
        "\n",
        "    #25. DNSRecord\n",
        "    dns = 1\n",
        "    try:\n",
        "        d = whois.whois(domain)\n",
        "    except:\n",
        "        dns=-1\n",
        "    if dns == -1:\n",
        "        data_set.append(-1)\n",
        "    else:\n",
        "        if registration_length / 365 <= 1:\n",
        "            data_set.append(-1)\n",
        "        else:\n",
        "            data_set.append(1)\n",
        "\n",
        "    #26. web_traffic\n",
        "    try:\n",
        "        rank = BeautifulSoup(urllib.request.urlopen(\"http://data.alexa.com/data?cli=10&dat=s&url=\" + url).read(), \"xml\").find(\"REACH\")['RANK']\n",
        "        rank= int(rank)\n",
        "        if (rank<100000):\n",
        "            data_set.append(1)\n",
        "        else:\n",
        "            data_set.append(0)\n",
        "    except TypeError:\n",
        "        data_set.append(-1)\n",
        "\n",
        "    #27. Page_Rank\n",
        "    try:\n",
        "        if global_rank > 0 and global_rank < 100000:\n",
        "            data_set.append(-1)\n",
        "        else:\n",
        "            data_set.append(1)\n",
        "    except:\n",
        "        data_set.append(1)\n",
        "\n",
        "    #28. Google_Index\n",
        "    site=search(url, 5)\n",
        "    if site:\n",
        "        data_set.append(1)\n",
        "    else:\n",
        "        data_set.append(-1)\n",
        "\n",
        "    #29. Links_pointing_to_page\n",
        "    if response == \"\":\n",
        "        data_set.append(-1)\n",
        "    else:\n",
        "        number_of_links = len(re.findall(r\"<a href=\", response.text))\n",
        "        if number_of_links == 0:\n",
        "            data_set.append(1)\n",
        "        elif number_of_links <= 2:\n",
        "            data_set.append(0)\n",
        "        else:\n",
        "            data_set.append(-1)\n",
        "\n",
        "    #30. Statistical_report\n",
        "    url_match=re.search('at\\.ua|usa\\.cc|baltazarpresentes\\.com\\.br|pe\\.hu|esy\\.es|hol\\.es|sweddy\\.com|myjino\\.ru|96\\.lt|ow\\.ly',url)\n",
        "    try:\n",
        "        ip_address=socket.gethostbyname(domain)\n",
        "        ip_match=re.search('146\\.112\\.61\\.108|213\\.174\\.157\\.151|121\\.50\\.168\\.88|192\\.185\\.217\\.116|78\\.46\\.211\\.158|181\\.174\\.165\\.13|46\\.242\\.145\\.103|121\\.50\\.168\\.40|83\\.125\\.22\\.219|46\\.242\\.145\\.98|'\n",
        "                           '107\\.151\\.148\\.44|107\\.151\\.148\\.107|64\\.70\\.19\\.203|199\\.184\\.144\\.27|107\\.151\\.148\\.108|107\\.151\\.148\\.109|119\\.28\\.52\\.61|54\\.83\\.43\\.69|52\\.69\\.166\\.231|216\\.58\\.192\\.225|'\n",
        "                           '118\\.184\\.25\\.86|67\\.208\\.74\\.71|23\\.253\\.126\\.58|104\\.239\\.157\\.210|175\\.126\\.123\\.219|141\\.8\\.224\\.221|10\\.10\\.10\\.10|43\\.229\\.108\\.32|103\\.232\\.215\\.140|69\\.172\\.201\\.153|'\n",
        "                           '216\\.218\\.185\\.162|54\\.225\\.104\\.146|103\\.243\\.24\\.98|199\\.59\\.243\\.120|31\\.170\\.160\\.61|213\\.19\\.128\\.77|62\\.113\\.226\\.131|208\\.100\\.26\\.234|195\\.16\\.127\\.102|195\\.16\\.127\\.157|'\n",
        "                           '34\\.196\\.13\\.28|103\\.224\\.212\\.222|172\\.217\\.4\\.225|54\\.72\\.9\\.51|192\\.64\\.147\\.141|198\\.200\\.56\\.183|23\\.253\\.164\\.103|52\\.48\\.191\\.26|52\\.214\\.197\\.72|87\\.98\\.255\\.18|209\\.99\\.17\\.27|'\n",
        "                           '216\\.38\\.62\\.18|104\\.130\\.124\\.96|47\\.89\\.58\\.141|78\\.46\\.211\\.158|54\\.86\\.225\\.156|54\\.82\\.156\\.19|37\\.157\\.192\\.102|204\\.11\\.56\\.48|110\\.34\\.231\\.42',ip_address)\n",
        "        if url_match:\n",
        "            data_set.append(-1)\n",
        "        elif ip_match:\n",
        "            data_set.append(-1)\n",
        "        else:\n",
        "            data_set.append(1)\n",
        "    except:\n",
        "        print ('Connection problem. Please check your internet connection!')\n",
        "\n",
        "\n",
        "    print (data_set)\n",
        "    return data_set\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import feature_extraction\n",
        "from sklearn.ensemble import RandomForestClassifier as rfc\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression as lr\n",
        "from flask import jsonify\n",
        "\n",
        "\n",
        "def getResult(url):\n",
        "\n",
        "    #Importing dataset\n",
        "    data = np.loadtxt(\"dataset.csv\", delimiter = \",\")\n",
        "\n",
        "    #Seperating features and labels\n",
        "    X = data[: , :-1]\n",
        "    y = data[: , -1]\n",
        "\n",
        "    #Seperating training features, testing features, training labels & testing labels\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
        "    clf = rfc()\n",
        "    clf.fit(X_train, y_train)\n",
        "    score = clf.score(X_test, y_test)\n",
        "    print(score*100)\n",
        "\n",
        "    X_new = []\n",
        "\n",
        "    X_input = url\n",
        "    X_new=feature_extraction.generate_data_set(X_input)\n",
        "    X_new = np.array(X_new).reshape(1,-1)\n",
        "\n",
        "    try:\n",
        "        prediction = clf.predict(X_new)\n",
        "        if prediction == -1:\n",
        "            return \"Phishing Url\"\n",
        "        else:\n",
        "            return \"Legitimate Url\"\n",
        "    except:\n",
        "        return \"Phishing Url\"\n"
      ],
      "metadata": {
        "id": "aK6Lki2ti7Lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import phishing_detection\n",
        "from flask import Flask\n",
        "from flask import (\n",
        "    Blueprint, flash, g, redirect, render_template, request, session, url_for\n",
        ")\n",
        "from flask import jsonify\n",
        "from werkzeug.utils import secure_filename\n",
        "app = Flask(__name__)\n",
        "\n",
        "UPLOAD_FOLDER= '/files'\n",
        "app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n",
        "ALLOWED_EXTENSIONS = set(['txt', 'pdf', 'png', 'jpg', 'jpeg', 'gif','py'])\n",
        "def allowed_file(filename):\n",
        "    return '.' in filename and \\\n",
        "           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
        "\n",
        "\n",
        "@app.route('/result')\n",
        "def result():\n",
        "    urlname  = request.args['name']\n",
        "    result  = phishing_detection.getResult(urlname)\n",
        "    return result\n",
        "\n",
        "# @app.route('/upload')\n",
        "# def upload():\n",
        "# \treturn 'yes'\n",
        "\n",
        "@app.route('/', methods = ['GET', 'POST'])\n",
        "def hello():\n",
        "\tif request.method == 'POST':\n",
        "\t\tif 'file' not in request.files:\n",
        "\t\t\tflash('no file part')\n",
        "\t\t\treturn \"false\"\n",
        "\t\tfile = request.files['file']\n",
        "\t\tif file.filename == '':\n",
        "\t\t\tflash('no select file')\n",
        "\t\t\treturn 'false'\n",
        "\t\tif file and allowed_file(file.filename):\n",
        "\t\t\tfilename = secure_filename(file.filename)\n",
        "\t\t\tcontents = file.read()\n",
        "\t\t\twith open(\"files/URL.txt\",\"wb\") as f:\n",
        "\t\t\t\tf.write(contents)\n",
        "\t\t\tfile.save = (os.path.join(app.config['UPLOAD_FOLDER'], filename))\n",
        "\t\t\treturn render_template(\"getInput.html\")\n",
        "\treturn  render_template(\"getInput.html\")\n",
        "\n",
        "\t\t\t\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "taXn4quKi-xs",
        "outputId": "b283063b-2385-4d8f-bc03-5a37d48f1032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
            " * Restarting with stat\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-whois"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yHrqxxDiod0",
        "outputId": "c1020f9f-5ffa-40c9-a2fe-76cbcd18974b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-whois\n",
            "  Downloading python-whois-0.7.3.tar.gz (91 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▋                            | 10 kB 17.6 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 20 kB 20.8 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 30 kB 24.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 40 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 51 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 61 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 71 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 81 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 91 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from python-whois) (0.16.0)\n",
            "Building wheels for collected packages: python-whois\n",
            "  Building wheel for python-whois (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-whois: filename=python_whois-0.7.3-py3-none-any.whl size=87721 sha256=0df0b660ef8d3b4ce300ecae9749a656082eaaa0f65a19a21f03f3575a470324\n",
            "  Stored in directory: /root/.cache/pip/wheels/11/05/f7/895ce5a73665f77c8274a7d55e34fb3e6b4abbb9a7637e215b\n",
            "Successfully built python-whois\n",
            "Installing collected packages: python-whois\n",
            "Successfully installed python-whois-0.7.3\n"
          ]
        }
      ]
    }
  ]
}